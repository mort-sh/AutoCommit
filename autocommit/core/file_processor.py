"""
Processes individual files or groups of files to prepare data for commit generation.
Handles hunk splitting, classification, and parallel processing orchestration.
"""

import os
import threading
from typing import Any

from autocommit.core.ai import OpenAIError, classify_hunks, generate_commit_message
from autocommit.core.config import Config
from autocommit.core.diff import split_diff_into_chunks
from autocommit.core.git_repository import GitRepository
from autocommit.core.message_generator import generate_messages_parallel  # Import from new location
from autocommit.core.patch_utils import (
    clean_patch_format,
    create_patch_for_group,
)  # Import from new location
from autocommit.utils.console import console


def _process_whole_file(
    file: dict[str, Any], config: Config, repo: GitRepository
) -> dict[str, Any] | None:
    """
    Process a single file as a whole, generate its commit message and patch data.
    Used for binary files, deleted files, or files with only one hunk/group.

    Args:
        file: File information including path, full diff patch, and status.
        config: The application configuration object.
        repo: The GitRepository instance (passed for consistency, not directly used here).

    Returns:
        A dictionary containing the commit data (message, patch, etc.),
        or None if processing fails.
    """
    path = file["path"]
    # This should now be the full diff patch relative to HEAD or deletion/binary marker
    full_diff_patch = file["diff"]
    status = file["status"]

    # Determine diff content for message generation (might differ from patch)
    # For binary/deleted, use the marker. Otherwise, use the patch content.
    diff_for_prompt = full_diff_patch
    if status.startswith("D"):
        diff_for_prompt = "File was deleted"
    elif file.get("is_binary", False):  # Check if marked as binary
        diff_for_prompt = "Binary file"

    try:
        message = generate_commit_message(diff_for_prompt, config.model)  # Use config.model

        # The patch content is simply the full diff we received for whole-file processing
        # For deleted files, the patch is generated by get_diff.
        # For binary files, staging happens via git add, so patch content is not strictly needed for apply. Mark it?
        # Let's store the full diff patch content here. If it's binary/deleted, _apply_commits handles it differently.
        patch_content_for_commit = (
            full_diff_patch if not file.get("is_binary", False) else None
        )  # Use None for binary

        # Return data instead of committing
        return {
            "group_index": 1,  # Only one group for whole file
            "total_groups": 1,
            "hunk_indices": [],  # Not applicable for whole file
            "message": message,
            "patch_content": patch_content_for_commit,  # Store the patch
            "num_hunks_in_group": 1,  # Treat whole file as one 'hunk' conceptually
            "total_hunks_in_file": 1,
            # Add status and is_binary flags to help commit_executor
            "status": status,
            "is_binary": file.get("is_binary", False),
        }

    except OpenAIError:
        # Error already logged by ai module
        return {
            "group_index": 1,
            "total_groups": 1,
            "hunk_indices": [],
            "message": "[Chore] Commit changes (AI Error)",
            "patch_content": None,
            "num_hunks_in_group": 1,
            "total_hunks_in_file": 1,
            "status": status,
            "is_binary": file.get("is_binary", False),
            "error": "AI Error",
        }
    except Exception as e:  # Catch other unexpected errors
        console.print(f"Unexpected error processing file {path}: {e}", style="warning")
        return {
            "group_index": 1,
            "total_groups": 1,
            "hunk_indices": [],
            "message": "[Chore] Commit changes (System Error)",
            "patch_content": None,
            "num_hunks_in_group": 1,
            "total_hunks_in_file": 1,
            "status": status,
            "is_binary": file.get("is_binary", False),
            "error": "System Error",
        }


def _process_file_hunks(
    file: dict[str, Any],
    config: Config,
    repo: GitRepository,  # Keep repo for potential future use, though not directly used now
) -> list[dict[str, Any]] | None:
    """
    Process hunks for a single file, group them, generate messages and patches.

    Args:
        file: File information including path, diff, and status.
        config: The application configuration object.
        repo: The GitRepository instance.

    Returns:
        List of dictionaries, each representing a commit group with its message and patch,
        or None if processing fails or no hunks are found.
    """
    path = file["path"]
    full_diff = file["diff"] # This should now be the full diff patch
    status = file["status"]  # Keep status for potential commit logic later

    # --- Extract Header ONCE --- (Moved header extraction here)
    header_lines = []
    lines = full_diff.splitlines()
    body_start_index = -1

    for i, line in enumerate(lines):
        if line.startswith("@@ "):
            body_start_index = i
            break
        if line.startswith("diff --git ") or line.startswith("index ") or \
           line.startswith("--- ") or line.startswith("+++ "):
            header_lines.append(line)

    file_header = "\n".join(header_lines)
    if file_header and not file_header.endswith("\n"):
        file_header += "\n"

    if not file_header and body_start_index != -1:
         console.print(f"[debug]Warning:[/] Could not extract standard header for [file_path]{path}[/]. Patch application might fail.", style="warning")
         # Create a minimal header if possible
         file_header = f"diff --git a/{path} b/{path}\n--- a/{path}\n+++ b/{path}\n"

    # --- Handle Binary/Deleted/Untracked --- (Check before hunk splitting)
    if status.startswith("D") or file.get("is_binary", False) or status == "??": # Consolidate checks
         result = _process_whole_file(file, config, repo)
         return [result] if result else None

    # --- Split into Hunks --- (Only if not binary/deleted/untracked)
    hunks = split_diff_into_chunks(full_diff, config.chunk_level)  # Use config.chunk_level

    if not hunks:
        # If hunk splitting failed but it's not untracked (should have been caught above)
        # treat as whole file (e.g., mode change only)
        if status != '??':
             console.print(f"No textual hunks found for modified file [file_path]{path}[/], treating as single change.", style="info")
             result = _process_whole_file(file, config, repo)
             return [result] if result else None
        # Should not happen for untracked, but handle defensively
        console.print(f"Warning: No hunks found for non-binary/deleted/untracked file [file_path]{path}[/]. Skipping.", style="warning")
        return None

    # --- Handle Single Hunk --- (If only one hunk found after splitting)
    if len(hunks) == 1:
        result = _process_whole_file(file, config, repo)
        return [result] if result else None

    # --- Classify Hunks --- (Only for files with multiple hunks)
    try:
        hunk_groups_indices = classify_hunks(hunks, config.model, config.debug)
        hunk_groups = []
        processed_indices = set()
        for group_indices in hunk_groups_indices:
            group = []
            current_group_indices = []
            for idx in group_indices:
                 if 0 <= idx < len(hunks) and idx not in processed_indices:
                    group.append(hunks[idx])
                    processed_indices.add(idx)
                    current_group_indices.append(idx)
            if group:
                 hunk_groups.append({"hunks": group, "indices": current_group_indices})
        remaining_hunks = []
        remaining_indices = []
        for idx, hunk in enumerate(hunks):
            if idx not in processed_indices:
                remaining_hunks.append(hunk)
                remaining_indices.append(idx)
        if remaining_hunks:
            if config.debug:
                 console.print(f"[debug]DEBUG:[/] Adding {len(remaining_hunks)} unclassified hunks to a separate group for [file_path]{path}[/].", style="debug")
            hunk_groups.append({"hunks": remaining_hunks, "indices": remaining_indices})

    except OpenAIError:
        console.print(
            f"Classification failed for {path}, treating as single group.", style="warning"
        )
        hunk_groups = [{"hunks": hunks, "indices": list(range(len(hunks)))}]
    except Exception as e:
        console.print(
            f"Unexpected error during hunk classification for {path}: {e}", style="warning"
        )
        hunk_groups = [{"hunks": hunks, "indices": list(range(len(hunks)))}]

    if not hunk_groups:
         console.print(f"Warning: Hunk classification resulted in empty groups for {path}. Treating as single group.", style="warning")
         hunk_groups = [{"hunks": hunks, "indices": list(range(len(hunks)))}]

    if config.debug:
        console.print(
            f"[debug]DEBUG:[/] Classified {len(hunks)} hunks into {len(hunk_groups)} groups for [file_path]{path}[/]. Group indices: {[g['indices'] for g in hunk_groups]}",
            style="debug",
        )

    # --- Prepare for Parallel Message Generation --- (Only for multi-group files)
    diffs_for_parallel_gen = [
        "\n".join(hunk["diff"] for hunk in group_info["hunks"])
        for group_info in hunk_groups
    ]

    # --- Generate All Messages for this File in Parallel --- (Uses message_generator)
    try:
        messages = generate_messages_parallel(
            diffs_for_parallel_gen, config.model, config.parallel # Removed chunk_level
        )
    except Exception as e:
        console.print(f"Error setting up parallel message generation for {path}: {e}", style="error")
        messages = ["[Chore] Commit changes (Parallel Setup Error)"] * len(hunk_groups)

    # --- Create Commit Data List --- (Assign generated messages)
    commit_data_list = []
    for i, group_info in enumerate(hunk_groups, 1):
        group = group_info["hunks"]
        group_hunk_indices = group_info["indices"]

        message = messages[i-1] if i <= len(messages) else "[Chore] Commit changes (Indexing Error)"

        # Create the patch BODY for this group using patch_utils
        group_patch_body = create_patch_for_group(group)

        if not group_patch_body:
             console.print(f"Warning: Could not generate patch body for group {i} in {path}. Skipping group.", style="warning")
             continue

        # Combine the extracted file_header with the group's patch body
        full_patch_for_group = file_header + group_patch_body # No extra newline needed if header has one

        # Clean the patch using patch_utils
        cleaned_patch = clean_patch_format(full_patch_for_group)

        commit_data_list.append({
            "group_index": i,
            "total_groups": len(hunk_groups),
            "hunk_indices": group_hunk_indices,
            "message": message,
            "patch_content": cleaned_patch, # Use the cleaned patch
            "num_hunks_in_group": len(group),
            "total_hunks_in_file": len(hunks),
             # Add status/binary info for consistency, though might be redundant here
            "status": status,
            "is_binary": file.get("is_binary", False)
        })

    return commit_data_list


def process_files_parallel(
    files: list[dict[str, Any]],
    config: Config,
    repo: GitRepository
) -> list[list[dict[str, Any]] | None]:
    """Processes a list of files in parallel, handling hunk processing and message generation.

    Args:
        files: List of file dictionaries to process.
        config: The application configuration.
        repo: The GitRepository instance.

    Returns:
        A list containing the processed commit data for each file.
        Each item in the list is either a list of commit group dictionaries
        or None if processing for that file failed.
        The order matches the input `files` list.
    """
    # Determine max workers based on parallel setting
    if config.parallel <= 0:
        # Auto mode: Use CPU count * 2 but limit based on files
        max_workers = min(len(files), (os.cpu_count() or 1) * 2) # Ensure os.cpu_count() returns at least 1
    else:
        # Use specified level but ensure we don't exceed files
        max_workers = min(len(files), config.parallel)

    threads = []
    active_threads = [] # Keep track of active threads
    file_queue = list(enumerate(files)) # Create a queue of files to process

    lock = threading.Lock()
    results = [(None)] * len(files) # Pre-allocate results list

    def worker():
        """Pulls files from queue and processes them using _process_file_hunks."""
        while True:
            with lock:
                if not file_queue:
                    return # No more files
                index, file = file_queue.pop(0)

            try:
                 # Process the file (handles whole file or hunks internally)
                 commit_data = _process_file_hunks(file, config, repo)
                 with lock:
                      results[index] = commit_data # Store result by original index
            except Exception as e:
                 # Log error and store None to indicate failure for this file
                 console.print(f"Error processing file {file.get('path', 'unknown')} in thread: {e}", style="warning")
                 with lock:
                      results[index] = None

    # Start worker threads
    for _ in range(max_workers):
        thread = threading.Thread(target=worker)
        thread.start()
        active_threads.append(thread)

    # Wait for all threads to complete
    for thread in active_threads:
        thread.join()

    # Results are already in order in the `results` list
    processed_data: list[list[dict[str, Any]] | None] = results
    return processed_data
